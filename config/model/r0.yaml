# B2T model configs

input_dims: 256

encoder:
  layers:
    # - ["mamba", input_channels, n_layer, bidirectional]
    # - ["resnet", in_dims, out_dims, stride, hidden_size]
    # - ["pooling", pooling_type, kernel_size, stride]
    # - ["attention", input_dims, n_layer, local_attn_window_size, positional_embeding]
    # - ["highway", input_dim, num_layers]
    # - ["conv", input_dims, output_dims, kernel_size, stride, groups]

    # depthwise conv
    # - ["resnet", 256, 512, 2]
    - ["conv", 256, 512, 5, 3, 1]
    - ["mamba", 512, 6, true]

decoder:
  phoneme_vocab_size: 46
  character_vocab_size: 29

  # not used by the ctc decoder
  layers:
    - ["mamba", 512, 2, true]
    - ["resnet", 512, 512, 2]
    - ["mamba", 512, 4, true]
  output_dims: 512


optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 2e-4
  last_lr: 2e-5
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08