# B2T model configs

input_dims: 256

encoder:
  layers:
    # - - "mamba"
    #   - input_channels: int
    #   - n_layer: int
    #   - bidirectional: bool

    # - - "concat"
    #   - input_dims
    #   - output_dims
    #   - group_size

    # - - "pooling"
    #   - pooling_type
    #   - group_size

    # - - "conv"
    #   - in_dims
    #   - out_dims
    #   - stride
    #   - hidden_size

    # - - "attention"
    #   - input_dims
    #   - n_layer
    #   - local_attn_window_size
    #   - positional_embeding

    - - "conv"
      - 256
      - 512
      - 2
      - 768

    - - "mamba"
      - 512
      - 8
      - false

decoder:
  phoneme_vocab_size: 46
  character_vocab_size: 29

  # not used by the ctc decoder
  layers:
    - - "mamba"
      - 512
      - 3
      - true


optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 1e-4
  last_lr: 5e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08