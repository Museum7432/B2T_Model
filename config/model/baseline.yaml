# B2T model configs

input_dims: 256

encoder:
  layers:
    # - ["mamba", input_channels, n_layer, bidirectional]
    # - ["t5", input_channels, n_layer]
    # - ["resnet", in_dims, out_dims, stride, hidden_size]
    # - ["pooling", pooling_type, kernel_size, stride]
    # - ["attention", input_dims, n_layer, local_attn_window_size, positional_embeding]
    # - ["highway", input_dim, num_layers]
    # - ["conv", input_dims, output_dims, kernel_size, stride, groups]
    # - ["lstm", input_size, num_layers, bidirectional]
    # - ["t5_conv_cross_att", input_dims, n_layer]

    - ["lstm", 256, 5, True]


decoders_conf:
  # train multiple decoders at once
  # name, type, loss weights
  # - ["ctc_al", "ctc_alphabet", 0.5]
  - ["ctc_ph", "ctc_phonetic", 0.5]
  # - ["dec_al", "decoder_alphabet", 0.3]
  # - ["dec_ph", "decoder_phonetic", 0.5]

decoders_layers:
  ctc_al:
    - ["mamba", 512, 2, true]
  ctc_ph:
    - ["mamba", 512, 1, true]
  dec_al:
    - ["mamba", 512, 1, true]
    - ["t5_dec", 512, 4]
  dec_ph:
    - ["mamba", 512, 2, true]
    - ["t5_dec", 512, 4, 8]


optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 3e-4
  last_lr: 1e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08