# B2T model configs

input_dims: 256

# use_people_speech: true

max_epochs: 10
word_level: false
val_check_interval: 0.5

encoder:
  layers:
    # - ["mamba", input_channels, n_layer, bidirectional]
    # - ["t5", input_channels, n_layer]
    # - ["resnet", in_dims, out_dims, stride, hidden_size]
    # - ["pooling", pooling_type, kernel_size, stride]
    # - ["attention", input_dims, n_layer, local_attn_window_size, positional_embeding]
    # - ["highway", input_dim, num_layers]
    # - ["conv", input_dims, output_dims, kernel_size, stride, groups]
    # - ["lstm", input_size, num_layers, bidirectional]
    # - ["t5_conv_cross_att", input_dims, n_layer]

    # - ["conv", 768, 768, 3, 2, 4]
    - ["conv", 256, 1024, 7, 2, 256]
    - ["highway", 1024, 2]
    - ["conv", 1024, 512, 3, 2, 1]
    - ["highway", 512, 2]
    - ["mamba", 512, 6, true]

decoders_conf:
  # train multiple decoders at once
  # name, type, loss weights
  - ["ctc_al", "ctc_alphabet", 0.5]
  - ["ctc_ph", "ctc_phonetic", 0.5]
  # - ["dec_al", "decoder_alphabet", 0.4]
  # - ["dec_ph", "decoder_phonetic", 0.25]

decoders_layers:
  ctc_al:
    - ["mamba", 512, 5, true]
  ctc_ph:
    - ["mamba", 512, 2, true]
  dec_al:
    # - ["mamba", 512, 1, true]
    - ["t5_dec", 512, 4]
  dec_ph:
    # - ["mamba", 512, 1, true]
    - ["t5_dec", 512, 5]


optimizer:
  warmup_perc: 0.2 # lr warmup for the first 10% of the training
  peak_lr: 1e-4
  last_lr: 1e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08