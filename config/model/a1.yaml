# B2T model configs

input_dims: 256

encoder:
  layers:
    # - - "mamba"
    #   - input_channels: int
    #   - n_layer: int
    #   - bidirectional: bool

    # - - "concat"
    #   - input_dims
    #   - output_dims
    #   - group_size

    # - - "pooling"
    #   - pooling_type
    #   - group_size

    # - - "conv"
    #   - in_dims
    #   - out_dims
    #   - stride

    # - - "conv"
    #   - 256
    #   - 256
    #   - 2

    - - "mamba"
      - 256
      - 3
      - true
      
    - - "conv"
      - 256
      - 256
      - 1
    
    - - "concat"
      - 256
      - 512
      - 2
    
    - - "mamba"
      - 512
      - 2
      - true
    
    - - "conv"
      - 512
      - 512
      - 1

    - - "concat"
      - 512
      - 512
      - 2
    
    - - "mamba"
      - 512
      - 6
      - true

  output_dims: 512

decoder:
  phoneme_vocab_size: 46
  character_vocab_size: 29

  # not used by the ctc decoder
  layers:
    - - "conv"
      - 512
      - 512
      - 1
    - - "concat"
      - 512
      - 512
      - 2
    - - "mamba"
      - 512
      - 4
      - true
  output_dims: 512

optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 2e-4
  last_lr: 5e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08
