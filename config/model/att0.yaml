# B2T model configs

input_dims: 256

encoder:
  layers:
    # - ["mamba", input_channels, n_layer, bidirectional]
    # - ["resnet", in_dims, out_dims, stride, hidden_size]
    # - ["pooling", pooling_type, kernel_size, stride]
    # - ["attention", input_dims, n_layer, local_attn_window_size, positional_embeding]
    # - ["highway", input_dim, num_layers]
    # - ["conv", input_dims, output_dims, kernel_size, stride, groups]

    # depthwise conv
    - ["conv", 256, 512, 3, 2, 1]
    - ["conv", 512, 512, 3, 2, 1]
    - ["attention", 512, 6, 64, false]


decoder:
  phoneme_vocab_size: 47
  character_vocab_size: 30

  # not used by the ctc decoder
  # layers:
  #   - ["conv", 512, 512, 3, 2, 1]

  t5_num_encoder_layers: 0
  t5_num_decoder_layers: 6

optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 1e-4
  last_lr: 2e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08