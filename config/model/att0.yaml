# B2T model configs

input_dims: 256

encoder:
  # the input sequence length will be reduced by 4 times
  # anything higher will mess with the ctc_loss
  # 2^(number of concat/pooling layers)
  input_reduction_ratio: 4
  layers:
    # - - "mamba"
    #   - input_channels: int
    #   - n_layer: int
    #   - bidirectional: bool

    # - - "concat"
    #   - input_dims
    #   - output_dims
    #   - group_size

    # - - "pooling"
    #   - pooling_type
    #   - group_size

    # - - "conv"
    #   - in_dims
    #   - out_dims
    #   - stride

    # - - "conv"
    #   - 256
    #   - 256
    #   - 2

    - - "mamba"
      - 256
      - 2
      - true
      
    - - "attention"
      - 256
      - 2
      - 32
      - false
    
    - - "concat"
      - 256
      - 256
      - 2
    
    - - "mamba"
      - 256
      - 2
      - true
    
    - - "attention"
      - 256
      - 2
      - 32
      - false

    - - "concat"
      - 256
      - 256
      - 2
    
    - - "mamba"
      - 256
      - 6
      - true

  output_dims: 256

decoder:
  phoneme_vocab_size: 46

optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 2e-4
  last_lr: 5e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08
