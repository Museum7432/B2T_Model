# B2T model configs

input_dims: 256

encoder:
  layers:
    # - ["mamba", input_channels, n_layer, bidirectional]
    # - ["t5", input_channels, n_layer]
    # - ["resnet", in_dims, out_dims, stride, hidden_size]
    # - ["pooling", pooling_type, kernel_size, stride]
    # - ["attention", input_dims, n_layer, local_attn_window_size, positional_embeding]
    # - ["highway", input_dim, num_layers]
    # - ["conv", input_dims, output_dims, kernel_size, stride, groups]


    - ["conv", 256, 1024, 7, 2, 256]
    - ["highway", 1024, 2]
    - ["conv", 1024, 512, 3, 2, 1]
    - ["highway", 768, 2]
    - ["mamba", 768, 6, true]

decoder:
  phoneme_vocab_size: 47
  character_vocab_size: 30

  # not used by the ctc decoder
  # layers:
  #   - ["mamba", 512, 2, true]
  #   - ["resnet", 512, 512, 2]
  #   - ["mamba", 512, 4, true]
  
  t5_num_encoder_layers: 0
  t5_num_decoder_layers: 6


optimizer:
  warmup_perc: 0.1 # lr warmup for the first 10% of the training
  peak_lr: 2e-4
  last_lr: 5e-6
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1e-08