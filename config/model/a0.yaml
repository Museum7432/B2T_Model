# B2T model configs

input_dims: 256
encoder:
  layers:
    - - conv
      - 256
      - 512
      - 1
    - - conv
      - 512
      - 512
      - 1
    - - conv
      - 512
      - 512
      - 1
    - - conv
      - 512
      - 512
      - 2
    - - conv
      - 512
      - 512
      - 1
    - - conv
      - 512
      - 512
      - 2
    - - mamba
      - 512
      - 1
      - true
    - - attention
      - 512
      - 5
      - 128
      - false
decoder:
  phoneme_vocab_size: 46
  character_vocab_size: 29
  layers:
    - - mamba
      - 512
      - 2
      - true
    - - conv
      - 512
      - 512
      - 2
      - 512
    - - mamba
      - 512
      - 4
      - true
  output_dims: 512
optimizer:
  warmup_perc: 0.1
  peak_lr: 0.0005
  last_lr: 5.0e-06
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 1.0e-08
